{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['image.cmap'] = 'jet'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "plt.rcParams['figure.figsize'] = (12, 10)\n",
    "plt.rcParams['axes.grid']=True\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',\n",
    "'xkcd:scarlet']\n",
    "\n",
    "\n",
    "class Obstacle:\n",
    "    FLAT, ROCK, HILL, MOUNTAIN = range(4)\n",
    "\n",
    "def load_image(file_path):\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "def tile_image(image, tile_size):\n",
    "    rows, cols = image.shape\n",
    "    tile_rows = rows // tile_size\n",
    "    tile_cols = cols // tile_size\n",
    "\n",
    "    tiles = []\n",
    "    for i in range(tile_rows):\n",
    "        for j in range(tile_cols):\n",
    "            tile = image[i*tile_size:(i+1)*tile_size, j*tile_size:(j+1)*tile_size]\n",
    "            tiles.append(tile)\n",
    "    return tiles, tile_rows, tile_cols\n",
    "\n",
    "def calculate_tile_statistics(tile):\n",
    "    stddev_height = np.std(tile)\n",
    "    mean_height = np.mean(tile)\n",
    "    return stddev_height, mean_height\n",
    "\n",
    "def classify_tile(stddev, mean, thresholds):\n",
    "    if stddev < thresholds['rock_stddev'] and mean < thresholds['rock_mean']:\n",
    "        return Obstacle.FLAT\n",
    "    elif thresholds['rock_stddev'] <= stddev < thresholds['hill_stddev']:\n",
    "        return Obstacle.ROCK\n",
    "    elif thresholds['hill_stddev'] <= stddev < thresholds['mountain_stddev']:\n",
    "        return Obstacle.HILL\n",
    "    else:\n",
    "        return Obstacle.MOUNTAIN\n",
    "\n",
    "def create_colored_overlay(image, tile_size, thresholds):\n",
    "    tiles, tile_rows, tile_cols = tile_image(image, tile_size)\n",
    "    overlay = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    colormap = {\n",
    "        Obstacle.FLAT: (0, 255, 0),\n",
    "        Obstacle.ROCK: (255, 255, 0),\n",
    "        Obstacle.HILL: (255, 0, 0),\n",
    "        Obstacle.MOUNTAIN: (0, 0, 255)\n",
    "    }\n",
    "\n",
    "    for tile_idx, tile in enumerate(tiles):\n",
    "        stddev, mean = calculate_tile_statistics(tile)\n",
    "        tile_type = classify_tile(stddev, mean, thresholds)\n",
    "        color = colormap[tile_type]\n",
    "        \n",
    "        row, col = divmod(tile_idx, tile_cols)\n",
    "        overlay[row*tile_size:(row+1)*tile_size, col*tile_size:(col+1)*tile_size] = color\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "def main(file_path, tile_size, thresholds, a1=0.7, a2=0.4):\n",
    "    image = load_image(file_path)\n",
    "    overlay = create_colored_overlay(image, tile_size, thresholds)\n",
    "    # Blend the original image and the overlay\n",
    "    blended = cv2.addWeighted(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), a1, overlay, a2, 0)\n",
    "\n",
    "    # Display the result with a legend\n",
    "    plt.figure(figsize=(10, 10), facecolor='black') # Set the figure's face color to black\n",
    "    plt.imshow(blended)\n",
    "    plt.title(f\"Obstacle Detection with Tile Size {tile_size}x{tile_size}\", color='white', fontweight='bold')\n",
    "\n",
    "    legend_labels = {\n",
    "        'Flat': (0, 255, 0),\n",
    "        'Rocks': (255, 255, 0),\n",
    "        'Hills': (255, 0, 0),\n",
    "        'Mountains': (0, 0, 255)\n",
    "    }\n",
    "    patches = [mpatches.Patch(color=np.array(color)/255, label=label) for label, color in legend_labels.items()]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1., 1), loc='upper left')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=2)  # This adjusts the padding around the plot area and between subplots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "thresholds = {\n",
    "    'rock_stddev': 3,          # example values that classify tiles as rocks\n",
    "    'rock_mean': 20,           # example mean height value\n",
    "    'hill_stddev': 8,         # example values that classify tiles as hills\n",
    "    'mountain_stddev': 15      # example values that classify tiles as mountains\n",
    "}\n",
    "\n",
    "def showframes_add(frames,ccmap= None,labels=[],label_font_size=8,nrows=0,ncols=0):\n",
    "    \"\"\"Sub function don't call it alone\n",
    "    \"\"\"\n",
    "\n",
    "    plot_di = int(frames.shape[0]**0.5)\n",
    "\n",
    "    if nrows ==0 or ncols ==0 :\n",
    "        nrows, ncols = plot_di,plot_di \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    \n",
    "    if len(labels)!=0:\n",
    "        plt.subplots_adjust(left=0.1,bottom=0.02,right=0.9,top=0.99,wspace=0.3,hspace=0.4) ### Fixes the postions of the plots to show labels\n",
    "        for ind ,ax in enumerate( axes.flat):\n",
    "            \n",
    "            im = ax.imshow(frames[int(ind)],cmap =ccmap) #### Plots the frame\n",
    "            ax.set_title(f\"lab: {labels[ind]}\", fontstyle='italic',fontsize =label_font_size , pad=2) #### Writes the labels\n",
    "            \n",
    "            ax.set_xticks([])#### Turn of Ticks\n",
    "            ax.set_yticks([])#### Turn of Ticks\n",
    "        \n",
    "            \n",
    "            \n",
    "                \n",
    "    else:\n",
    "        plt.subplots_adjust(left=0.1,bottom=0.02,right=0.9,top=0.9,wspace=0.3,hspace=0.4)  ### Fixes the postions of the plots\n",
    "        for ind ,ax in enumerate( axes.flat):\n",
    "\n",
    "            im = ax.imshow(frames[int(ind)],cmap =ccmap) #### Plots the frame\n",
    "            ax.set_title(f\"{ind}\", fontstyle='italic',fontsize = label_font_size, pad=2)  #### Writes the frame number\n",
    "            ax.set_xticks([])#### Turn of Ticks\n",
    "            ax.set_yticks([])#### Turn of Ticks\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.9, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    \n",
    "    \n",
    "def showframes(frames,typee= None,fig_s = (10,10),labels: list =[],label_font_size: int =8,nrows=0,ncols=0):\n",
    "     \n",
    "    \"\"\"good for dealing with many frames with different image types like RGB, BGR, GRAY... \n",
    "    with some types \"cmaps\":\n",
    "    gist_gray = g\n",
    "    jet = c\n",
    "    tab20b = t\n",
    "    viridis = v\n",
    "    cividis = d\n",
    "    BGR = bgr\n",
    "    or leave it and will do defult\n",
    "\n",
    "    labels are used to make titles for each image like the model prediction for this image takes a listlike \n",
    "\n",
    "    label_font_size takes int \n",
    "\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = fig_s\n",
    "    figure(figsize=fig_s, dpi=100)\n",
    "    \n",
    "    \n",
    "    if typee==\"g\":\n",
    "        showframes_add(frames,ccmap ='gist_gray',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "    elif typee==\"c\" :\n",
    "        showframes_add(frames,ccmap ='jet',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "    elif typee==\"t\" :\n",
    "        showframes_add(frames,ccmap ='tab20b',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "    elif typee==\"v\" :\n",
    "        showframes_add(frames,ccmap ='viridis',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "    elif typee==\"d\" :\n",
    "        showframes_add(frames,ccmap ='cividis',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "\n",
    "    elif typee==\"RGB\"or typee=='rgb':\n",
    "        showframes_add(frames[:,:,:,[2,1,0]],ccmap ='cividis',labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "    else:\n",
    "        showframes_add(frames,labels=labels,label_font_size= label_font_size,nrows=nrows,ncols=ncols)\n",
    "        \n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def index_to_position(index, tile_cols):\n",
    "    \"\"\"Convert a linear index to a 2D position (row, col).\"\"\"\n",
    "    row = index // tile_cols\n",
    "    col = index % tile_cols\n",
    "    return row, col\n",
    "\n",
    "def position_to_index(row, col, tile_cols):\n",
    "    \"\"\"Convert a 2D position (row, col) back to a linear index.\"\"\"\n",
    "    return row * tile_cols + col\n",
    "\n",
    "def label_tiles(tiles, tile_rows, tile_cols):\n",
    "    \"\"\"Label tiles with their linear index based on their position.\"\"\"\n",
    "    labeled_tiles = []\n",
    "    for index, tile in enumerate(tiles):\n",
    "        row, col = index_to_position(index, tile_cols)\n",
    "        label = f\"tile_{row}_{col}\"\n",
    "        labeled_tiles.append(label)\n",
    "    return labeled_tiles\n",
    "\n",
    "def extract_position_from_label(label):\n",
    "    \"\"\"Extract the row and column number from a tile label.\"\"\"\n",
    "    # Assume label format is 'tile_row_column'\n",
    "    parts = label.split('_')\n",
    "    if len(parts) == 3 and parts[0] == 'tile':\n",
    "        row, col = int(parts[1]), int(parts[2])\n",
    "        return row, col\n",
    "    else:\n",
    "        raise ValueError(\"Label does not match the expected format 'tile_row_col'\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def save_labeled_tiles(image, tile_size, thresholds, storage_path):\n",
    "    os.makedirs(storage_path, exist_ok=True)  # Ensure base storage path exists\n",
    "\n",
    "    # Labels for the folders\n",
    "    labels = {\n",
    "        Obstacle.FLAT: \"Flat\",\n",
    "        Obstacle.ROCK: \"Rock\",\n",
    "        Obstacle.HILL: \"Hill\",\n",
    "        Obstacle.MOUNTAIN: \"Mountain\"\n",
    "    }\n",
    "\n",
    "    # Create labeled folders\n",
    "    for label in labels.values():\n",
    "        os.makedirs(os.path.join(storage_path, label), exist_ok=True)\n",
    "\n",
    "    # Process and save the tiles\n",
    "    tiles, tile_rows, tile_cols = tile_image(image, tile_size)\n",
    "    for tile_idx, tile in enumerate(tiles):\n",
    "        stddev, mean = calculate_tile_statistics(tile)\n",
    "        tile_type = classify_tile(stddev, mean, thresholds)\n",
    "        label = labels[tile_type]\n",
    "        label_folder = os.path.join(storage_path, label)\n",
    "\n",
    "        # Save the tile image to the respective folder\n",
    "        tile_filename = f\"tile_{tile_idx}.png\"\n",
    "        tile_path = os.path.join(label_folder, tile_filename)\n",
    "        cv2.imwrite(tile_path, tile)\n",
    "\n",
    "tile_size=200\n",
    "# Example usage\n",
    "# Define thresholds, etc.\n",
    "storage_path = 'data/combiled'  # Replace with your actual storage path\n",
    "image = load_image('data/JEZ_ctx_B_soc_008_DTM_MOLAtopography_DeltaGeoid_20m_Eqc_latTs0_lon0_.jpg')  # Load the main image from which to extract tiles\n",
    "save_labeled_tiles(image, tile_size, thresholds, storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "tile_size =50 \n",
    "image = load_image('data/JEZ_ctx_B_soc_008_DTM_MOLAtopography_DeltaGeoid_20m_Eqc_latTs0_lon0_.jpg')  \n",
    "\n",
    "labels=label_tiles(*tile_image(image,tile_size))\n",
    "\n",
    "pos_s = [extract_position_from_label(l) for l in labels]\n",
    "print(pos_s[:5])\n",
    "y = [position_to_index(*pos,tile_image(image,tile_size)[2]) for pos in pos_s]\n",
    "\n",
    "\n",
    "data = np.array(tile_image(image,tile_size)[0])\n",
    "\n",
    "labels=label_tiles(*tile_image(image,tile_size))\n",
    "# showframes(data[:5],labels=y[:5],label_font_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class Augmentor:\n",
    "    def __init__(self, rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, \n",
    "                 shear_range=0.2, zoom_range=0.2, horizontal_flip=True, noise_range=0.01):\n",
    "        self.rotation_range = rotation_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.height_shift_range = height_shift_range\n",
    "        self.shear_range = shear_range\n",
    "        self.zoom_range = zoom_range\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.noise_range = noise_range\n",
    "\n",
    "        # Initialize ImageDataGenerator with the passed parameters\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=self.rotation_range,\n",
    "            width_shift_range=self.width_shift_range,\n",
    "            height_shift_range=self.height_shift_range,\n",
    "            shear_range=self.shear_range,\n",
    "            zoom_range=self.zoom_range,\n",
    "            horizontal_flip=self.horizontal_flip,\n",
    "            preprocessing_function=self.add_noise\n",
    "        )\n",
    "\n",
    "    def add_noise(self, image):\n",
    "        \"\"\"Apply random noise to an image.\"\"\"\n",
    "        variance = np.random.uniform(0, self.noise_range) * (np.max(image) - np.min(image))\n",
    "        noise = np.random.normal(0, variance, image.shape)\n",
    "        noisy_image = image + noise\n",
    "        noisy_image = np.clip(noisy_image, 0, 255)\n",
    "        return noisy_image.astype(image.dtype)\n",
    "\n",
    "    def flow(self, x, y, batch_size=32):\n",
    "        \"\"\"Generate batches of augmented data.\"\"\"\n",
    "        return self.datagen.flow(x, y, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming the Augmentor class is already defined as before\n",
    "augmentor = Augmentor(rotation_range=350, width_shift_range=0.15, noise_range=0.04)\n",
    "\n",
    "# Example dataset (replace data and y with your actual data)\n",
    "# data = np.array([...])  # Your image data\n",
    "# y = np.array([...])  # Your labels\n",
    "\n",
    "# Factor to determine the number of augmented images per original image\n",
    "augment_factor = 100  # Change this number based on how many augmented images you want per original image\n",
    "\n",
    "# Lists to store augmented images and labels\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "# This 'for' loop will generate 'augment_factor' augmented images for each original image\n",
    "for i in range(len(data)):\n",
    "    # Get a single image and label\n",
    "    image = data[i]\n",
    "    label = y[i]\n",
    "\n",
    "    # Expand the image dimensions if necessary (add the channels dimension)\n",
    "    if image.ndim == 2: # for grayscale images\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "    \n",
    "    # Augment the image 'augment_factor' times\n",
    "    for _ in range(augment_factor):\n",
    "        # Use the 'flow' function from the Augmentor, which expects a batch\n",
    "        # Ensure the image has four dimensions (batch_size, height, width, channels)\n",
    "        image_batch, label_batch = next(\n",
    "            augmentor.flow(\n",
    "                np.expand_dims(image, 0), # add the batch dimension\n",
    "                np.expand_dims(label, 0), # add the batch dimension for the labels if necessary\n",
    "                batch_size=1\n",
    "            )\n",
    "        )\n",
    "        # Remove the batch dimension since we are processing one image at a time\n",
    "        augmented_image = np.squeeze(image_batch, axis=0)\n",
    "        augmented_label = np.squeeze(label_batch, axis=0)\n",
    "        \n",
    "        # Append the augmented images and labels\n",
    "        augmented_images.append(augmented_image)\n",
    "        augmented_labels.append(augmented_label)\n",
    "\n",
    "# ...\n",
    "\n",
    "# Convert the lists to Numpy arrays\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# 'augmented_images' and 'augmented_labels' now contain the augmented dataset\n",
    "# showframes(augmented_images[:5],labels=augmented_labels[:5])\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        augmented_images, augmented_labels, test_size=0.25, random_state=42,shuffle=True)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByAccuracy(Callback):\n",
    "    def __init__(self, monitor='accuracy', value=0.95, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_accuracy = logs.get(self.monitor)\n",
    "        if current_accuracy is None:\n",
    "            raise ValueError(f\"Metric `{self.monitor}` is not available. Available metrics are: {', '.join(list(logs.keys()))}\")\n",
    "\n",
    "        if current_accuracy >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Epoch {epoch}: early stopping with {self.monitor} = {current_accuracy}\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback with the desired accuracy threshold\n",
    "early_stopping_callback = EarlyStoppingByAccuracy(monitor='val_accuracy', value=0.70, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "446/446 [==============================] - 13s 16ms/step - loss: 7.5760 - accuracy: 0.0023 - val_loss: 6.9779 - val_accuracy: 0.0048\n",
      "Epoch 2/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 6.7869 - accuracy: 0.0073 - val_loss: 6.4476 - val_accuracy: 0.0137\n",
      "Epoch 3/100\n",
      "446/446 [==============================] - 6s 14ms/step - loss: 6.2842 - accuracy: 0.0193 - val_loss: 5.9664 - val_accuracy: 0.0308\n",
      "Epoch 4/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 5.5753 - accuracy: 0.0450 - val_loss: 5.4367 - val_accuracy: 0.0473\n",
      "Epoch 5/100\n",
      "446/446 [==============================] - 6s 15ms/step - loss: 4.7113 - accuracy: 0.0850 - val_loss: 4.3412 - val_accuracy: 0.1085\n",
      "Epoch 6/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 3.9994 - accuracy: 0.1428 - val_loss: 3.6603 - val_accuracy: 0.1696\n",
      "Epoch 7/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 3.4653 - accuracy: 0.1979 - val_loss: 3.2609 - val_accuracy: 0.2204\n",
      "Epoch 8/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 3.0742 - accuracy: 0.2488 - val_loss: 2.9106 - val_accuracy: 0.2764\n",
      "Epoch 9/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 2.7104 - accuracy: 0.3064 - val_loss: 3.1206 - val_accuracy: 0.2294\n",
      "Epoch 10/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 2.4459 - accuracy: 0.3552 - val_loss: 2.3232 - val_accuracy: 0.3721\n",
      "Epoch 11/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 2.2338 - accuracy: 0.3980 - val_loss: 2.3253 - val_accuracy: 0.3648\n",
      "Epoch 12/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 2.0255 - accuracy: 0.4416 - val_loss: 2.0753 - val_accuracy: 0.4165\n",
      "Epoch 13/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.8975 - accuracy: 0.4758 - val_loss: 1.7875 - val_accuracy: 0.4900\n",
      "Epoch 14/100\n",
      "446/446 [==============================] - 7s 16ms/step - loss: 1.7145 - accuracy: 0.5120 - val_loss: 2.0208 - val_accuracy: 0.4479\n",
      "Epoch 15/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.6311 - accuracy: 0.5303 - val_loss: 1.8015 - val_accuracy: 0.4954\n",
      "Epoch 16/100\n",
      "446/446 [==============================] - 7s 16ms/step - loss: 1.5645 - accuracy: 0.5569 - val_loss: 1.5606 - val_accuracy: 0.5536\n",
      "Epoch 17/100\n",
      "446/446 [==============================] - 7s 16ms/step - loss: 1.3911 - accuracy: 0.5911 - val_loss: 1.5254 - val_accuracy: 0.5615\n",
      "Epoch 18/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.3154 - accuracy: 0.6120 - val_loss: 1.5037 - val_accuracy: 0.5565\n",
      "Epoch 19/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.2729 - accuracy: 0.6201 - val_loss: 1.4754 - val_accuracy: 0.5715\n",
      "Epoch 20/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.1840 - accuracy: 0.6456 - val_loss: 2.0806 - val_accuracy: 0.4921\n",
      "Epoch 21/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.1355 - accuracy: 0.6579 - val_loss: 1.3548 - val_accuracy: 0.6032\n",
      "Epoch 22/100\n",
      "446/446 [==============================] - 7s 16ms/step - loss: 1.0502 - accuracy: 0.6832 - val_loss: 1.1399 - val_accuracy: 0.6672\n",
      "Epoch 23/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.0018 - accuracy: 0.6951 - val_loss: 1.1654 - val_accuracy: 0.6629\n",
      "Epoch 24/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.2202 - accuracy: 0.6697 - val_loss: 1.2519 - val_accuracy: 0.6351\n",
      "Epoch 25/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 0.8974 - accuracy: 0.7268 - val_loss: 1.0423 - val_accuracy: 0.6939\n",
      "Epoch 26/100\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 0.8640 - accuracy: 0.7344 - val_loss: 1.3299 - val_accuracy: 0.6253\n",
      "Epoch 27/100\n",
      "443/446 [============================>.] - ETA: 0s - loss: 1.0122 - accuracy: 0.7136Epoch 26: early stopping with val_accuracy = 0.7139999866485596\n",
      "446/446 [==============================] - 7s 15ms/step - loss: 1.0113 - accuracy: 0.7137 - val_loss: 0.9867 - val_accuracy: 0.7140\n",
      "1188/1188 [==============================] - 5s 4ms/step - loss: 0.9867 - accuracy: 0.7140\n",
      "Test accuracy: 0.7139999866485596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Data Preparation\n",
    "\n",
    "# Step 2: Model Architecture Design\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(tile_size, tile_size, 1)),  # Add a '1' for grayscale images\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(len(y), activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 3: Model Training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=100, validation_data=[test_data, test_labels],batch_size=256,callbacks=[early_stopping_callback])\n",
    "\n",
    "# Step 4: Evaluation and Validation\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Step 5: Deployment and Testing\n",
    "# Deploy and test the trained model in a real or simulated environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in HDF5 format\n",
    "model.save('model/saved_model_acc_71.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
